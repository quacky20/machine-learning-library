{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import copy, math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normal(x):\n",
    "        mu = np.mean(x,axis=0)\n",
    "        sigma = np.std(x,axis=0)\n",
    "        x_norm = (x-mu)/sigma\n",
    "\n",
    "        return (x_norm)\n",
    "        \n",
    "def mean_normal(x):\n",
    "        mu = np.mean(x,axis=0)\n",
    "        x_norm = (x-mu)/(np.max(x)-np.min(x))\n",
    "                \n",
    "        return (x_norm)\n",
    "\n",
    "def r2_score(y,y_pred):\n",
    "        y_avg = np.average(y, axis=0)\n",
    "        num = 0\n",
    "        den = 0\n",
    "        for i in range(y.shape[0]):\n",
    "                num = num + (y[i]-y_pred[i])**2\n",
    "                den = den + (y[i]-y_avg)**2\n",
    "        return (1 - (num/den))\n",
    "\n",
    "def mode(row):\n",
    "        unique_ele, count = np.unique(row, return_counts=True)\n",
    "        max_idx = np.argmax(count)\n",
    "        mode = unique_ele[max_idx]\n",
    "\n",
    "        return mode\n",
    "\n",
    "def accuracy(y_pred, y_true):\n",
    "        c = 0\n",
    "        for i in range (y_true.shape[0]):\n",
    "                if y_true[i] == y_pred[i]:\n",
    "                        c += 1\n",
    "                \n",
    "        return (c/y_true.shape[0])\n",
    "        \n",
    "def plot_cost(Cost_list, iterations):\n",
    "        t = np.arange(0, iterations)\n",
    "        plt.plot(t, Cost_list)\n",
    "        plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "\n",
    "    def __init__(self,x,w,b,l,alpha,epochs):\n",
    "        self.x = x\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.l = l\n",
    "        self.alpha = alpha\n",
    "        self.epochs = epochs\n",
    "\n",
    "\n",
    "    def cost(self,y,w,b,l):\n",
    "        m = self.x.shape[0]\n",
    "        f_wb = np.dot(self.x,w) + b\n",
    "        cost = (np.sum(np.square(f_wb-y))/(2*m)) + (l/(2*m)) * np.sum(np.square(w))\n",
    "        return cost\n",
    "    \n",
    "    def grad_fn (self,y,w,b,l):\n",
    "        m = self.x.shape[0]\n",
    "        f_wb = np.dot(self.x, w) + b\n",
    "        err = f_wb - y\n",
    "        dj_dw = (np.dot(err, self.x) / m) + (l/m)*w\n",
    "        dj_db = np.sum(err) / m\n",
    "        return dj_dw, dj_db\n",
    "    \n",
    "    def grad_descent(self,y):\n",
    "        J_history = []\n",
    "        l = self.l\n",
    "        _w = copy.deepcopy(self.w)\n",
    "        _b = self.b\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "\n",
    "            dj_dw,dj_db=self.grad_fn(y,_w,_b,l)\n",
    "\n",
    "            _w = _w - self.alpha * dj_dw\n",
    "            _b = _b - self.alpha * dj_db\n",
    "\n",
    "            if i<100000:\n",
    "                J_history.append(self.cost(y,_w,_b,l))\n",
    "\n",
    "            if i% math.ceil(self.epochs / 10) == 0:\n",
    "                print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.5f}   \")\n",
    "        \n",
    "        return _w, _b, J_history\n",
    "    \n",
    "    def predict(self,x,w,b):\n",
    "        return (np.dot(x,w)+b)\n",
    "    \n",
    "class PolynomialRegression(LinearRegression):\n",
    "\n",
    "    def __init__(self,x,w,b,l,alpha,epochs):\n",
    "        self.x = x\n",
    "        self.w = w\n",
    "        self.b = b\n",
    "        self.l = l\n",
    "        self.alpha = alpha\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def features(self):\n",
    "        m = self.x.shape[1]\n",
    "        data = pd.DataFrame(np.zeros((self.x.shape[0],self.n*m)))\n",
    "        \n",
    "        for j in range(m):   \n",
    "            for i in range(1,self.n+1):  \n",
    "                data.iloc[:,j+((i-1)*m)] = (self.x[:,j]**(i)).reshape(-1,1)\n",
    "        x_poly = np.array(data.values.tolist())\n",
    "        return x_poly\n",
    "\n",
    "    def features_interacting(self):\n",
    "        m = self.x.shape[1]\n",
    "        data = pd.DataFrame(np.zeros((self.x.shape[0],(self.n+2)*m)))\n",
    "        \n",
    "        for j in range(m):   \n",
    "            for i in range(1,self.n+3):  \n",
    "                if i == self.n+1:\n",
    "                    data.iloc[:,j+((i-1)*m)] = (self.x[:,j]*self.x[:,m-j-1]).reshape(-1,1)\n",
    "                elif i == self.n+2:\n",
    "                    data.iloc[:,j+((i-1)*m)] = ((self.x[:,j]**(self.n-1))*self.x[:,m-j-1]).reshape(-1,1)\n",
    "                else:    \n",
    "                    data.iloc[:,j+((i-1)*m)] = (self.x[:,j]**(i)).reshape(-1,1)\n",
    "        x_poly = np.array(data.values.tolist())\n",
    "        return x_poly\n",
    "    \n",
    "class LogisticRegression:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def sigmoid(self,x):\n",
    "        sig = 1/(1+(np.exp(-x)))\n",
    "        return sig\n",
    "    \n",
    "    def cost(self,x,y,w,b):\n",
    "        m = x.shape[0]\n",
    "        f_wb = self.sigmoid(np.dot(x,w) + b)\n",
    "        cost = -np.sum(y * np.log(f_wb) + (1-y) * np.log(1 - f_wb)) / m\n",
    "        return cost\n",
    "    \n",
    "    def grad_fn(self,x,y,w,b):\n",
    "        m = x.shape[0]\n",
    "        f_wb = self.sigmoid(np.dot(x,w) + b)\n",
    "        err = f_wb - y\n",
    "        dj_dw = np.dot(err, x) / m\n",
    "        dj_db = np.sum(err) / m\n",
    "        return dj_dw, dj_db\n",
    "    \n",
    "    def grad_descent(self,x,y,w_in,b_in,alpha,epoch):\n",
    "        J_history = []\n",
    "        w = copy.deepcopy(w_in)\n",
    "        b = b_in\n",
    "        for i in range(epoch):\n",
    "\n",
    "            dj_dw,dj_db=self.grad_fn(x,y,w,b)\n",
    "\n",
    "            w = w - alpha * dj_dw\n",
    "            b = b - alpha * dj_db\n",
    "\n",
    "            if i<100000:\n",
    "                J_history.append(self.cost(x, y, w, b))\n",
    "\n",
    "            if i% math.ceil(epoch / 10) == 0:\n",
    "                print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.5f}   \")\n",
    "        \n",
    "        return w, b, J_history\n",
    "    \n",
    "    def predict(self,x,w,b):\n",
    "        y_pred = np.zeros(x.shape[0])\n",
    "        for i in range(x.shape[0]):\n",
    "            y_pred[i] = self.sigmoid(np.dot(x[i],w)+b)\n",
    "        return y_pred\n",
    "    \n",
    "    def fitnew(self,x,y,w_in,b_in,alpha,epochs):\n",
    "        print(\"Fitting the given dataset..\")\n",
    "        self.theta = []\n",
    "        self.cost_ = []\n",
    "        w = copy.deepcopy(w_in)\n",
    "        b = b_in\n",
    "        # x = np.insert(x, 0, 1, axis=1)\n",
    "        m = len(y)\n",
    "        for i in np.unique(y):\n",
    "            costx = []\n",
    "            y_onevsall = np.where(y == i, 1, 0)\n",
    "            # costlist = []\n",
    "            for _ in range(epochs):\n",
    "                dj_dw,dj_db=self.grad_fn(x,y_onevsall,w,b)\n",
    "\n",
    "                w = w - alpha * dj_dw\n",
    "                b = b - alpha * dj_db\n",
    "            \n",
    "                cost = self.cost(x,y_onevsall,w,b)\n",
    "            print(f\"For {i:4f}: Cost {cost:8.5f}   \")        \n",
    "            self.theta.append((w, i))\n",
    "            # self.cost.append((costlist,i))\n",
    "        \n",
    "        return self.theta, b\n",
    "    \n",
    "    def predictnew(self, X, w, b):\n",
    "        # X = np.insert(X, 0, 1, axis=1)\n",
    "        X_predicted = [max((self.sigmoid(i.dot(theta) + b), c) for theta, c in w)[1] for i in X ]\n",
    "\n",
    "        return X_predicted\n",
    "\n",
    "\n",
    "def multinomial_features(x, n):\n",
    "        m = x.shape[0]\n",
    "\n",
    "        terms = math.comb(n + 2, 2)\n",
    "        features = np.zeros((m, terms + 3))\n",
    "\n",
    "        for i in range(m):\n",
    "            x1, x2, x3 = x[i]\n",
    "            terms = [x1**m1 * x2**m2 * x3**m3 for m1 in range(n + 1) for m2 in range(n + 1) for m3 in range(n + 1) if m1 + m2 + m3 == n]\n",
    "            features[i, :len(terms)] = terms\n",
    "            features[i, -3:] = [x1, x2, x3]  # Add x1, x2, x3 as the last three columns\n",
    "\n",
    "        return features\n",
    "    \n",
    "# def multinomial_features(x, n):\n",
    "#     m = x.shape[0]\n",
    "#     features_count = sum([math.comb(n + i - 1, i) for i in range(1, n + 2)])\n",
    "        \n",
    "#     features = np.zeros((m, features_count))\n",
    "\n",
    "#     for i in range(m):\n",
    "#         current_index = 0\n",
    "\n",
    "#         for j in range(x.shape[1]):\n",
    "#             features[i, current_index] = x[i, j]\n",
    "#             current_index += 1\n",
    "\n",
    "#         for d in range(2, n + 1):\n",
    "#             for m1 in range(d + 1):\n",
    "#                 for m2 in range(d - m1 + 1):\n",
    "#                     m3 = d - m1 - m2\n",
    "#                     features[i, current_index] = x[i, 0]**m1 * x[i, 1]**m2 * x[i, 2]**m3\n",
    "#                     current_index += 1\n",
    "\n",
    "#     return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('Lineardata_train.csv','r')\n",
    "data=list(csv.reader(file))\n",
    "x=[]\n",
    "y=[]\n",
    "for row in data:\n",
    "    y.append(row[0])\n",
    "for row in data:\n",
    "    x.append(row[1:])\n",
    "file.close()\n",
    "x_train=x[1:24001]\n",
    "y_train=y[1:24001]\n",
    "x_train = np.array(x_train,dtype=float)\n",
    "y_train = np.array(y_train,dtype=float)\n",
    "y_train = y_train.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 64100376.68833   \n",
      "Iteration 1000: Cost  0.13959   \n",
      "Iteration 2000: Cost  0.00506   \n",
      "Iteration 3000: Cost  0.00506   \n",
      "Iteration 4000: Cost  0.00506   \n",
      "Iteration 5000: Cost  0.00506   \n",
      "Iteration 6000: Cost  0.00506   \n",
      "Iteration 7000: Cost  0.00506   \n",
      "Iteration 8000: Cost  0.00506   \n",
      "Iteration 9000: Cost  0.00506   \n"
     ]
    }
   ],
   "source": [
    "initial_w = np.zeros(x_train.shape[1])\n",
    "initial_b = 0\n",
    "\n",
    "x_train = zscore_normal(x_train)\n",
    "\n",
    "iterations = 10000\n",
    "alpha = 0.01\n",
    "l = 10e-12\n",
    "\n",
    "linear_model = LinearRegression(x_train,initial_w,initial_b,l,alpha,iterations)\n",
    "\n",
    "w_final, b_final, J_hist = linear_model.grad_descent(y_train)\n",
    "\n",
    "# print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "\n",
    "y_pred = linear_model.predict(x_train,w_final,b_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score:  0.9999999999225841\n"
     ]
    }
   ],
   "source": [
    "r2score = r2_score(y_train,y_pred)\n",
    "print(\"R2 Score for train data: \",r2score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_cost(J_hist, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=x[24001:]\n",
    "y_test=y[24001:]\n",
    "x_test = np.array(x_test,dtype=float)\n",
    "y_test = np.array(y_test,dtype=float)\n",
    "y_test = y_test.transpose()\n",
    "x_test = zscore_normal(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score for test data:  0.9919622618692064\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = linear_model.predict(x_test, w_final, b_final)\n",
    "print(\"R2 Score for test data: \",r2_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('Polynomialdata_train.csv','r')\n",
    "data=list(csv.reader(file))\n",
    "x=[]\n",
    "y=[]\n",
    "for row in data:\n",
    "    y.append(row[0])\n",
    "for row in data:\n",
    "    x.append(row[1:])\n",
    "file.close()\n",
    "x_train=x[1:24001]\n",
    "y_train=y[1:24001]\n",
    "x_train = np.array(x_train,dtype=float)\n",
    "y_train = np.array(y_train,dtype=float)\n",
    "y_train = y_train.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 50345047206261.52344   \n",
      "Iteration 1000: Cost 6089837.81863   \n",
      "Iteration 2000: Cost 518.73033   \n",
      "Iteration 3000: Cost 404.28068   \n",
      "Iteration 4000: Cost 404.27795   \n",
      "Iteration 5000: Cost 404.27795   \n",
      "Iteration 6000: Cost 404.27795   \n",
      "Iteration 7000: Cost 404.27795   \n",
      "Iteration 8000: Cost 404.27795   \n",
      "Iteration 9000: Cost 404.27795   \n",
      "b,w found by gradient descent: 1756232.66,[8.18748204e+06 9.09868589e+05 9.58485317e+04 8.14448665e+03\n",
      " 5.42152270e+02 2.74996651e+01 9.53302352e-01 6.66340413e+06\n",
      " 5.78974032e+05 5.63013910e+04 4.25999484e+03 2.54200710e+02\n",
      " 1.12519178e+01 3.37047446e+06 3.05409936e+05 2.58456618e+04\n",
      " 1.68503788e+03 7.57562488e+01 1.30173083e+06 1.18937329e+05\n",
      " 8.45575588e+03 4.17695615e+02 4.18163930e+05 3.40660153e+04\n",
      " 1.92753443e+03 1.18107290e+05 8.42188709e+03 2.86914847e+04\n",
      " 1.63150648e+01 2.23852091e-01 3.58875350e-01] \n"
     ]
    }
   ],
   "source": [
    "degree = 6\n",
    "\n",
    "w=np.zeros([math.comb(degree+2, 2)+3])\n",
    "b=0\n",
    "iterations = 10000\n",
    "alpha = 0.05\n",
    "l = 10e-12\n",
    "\n",
    "# x = regressor.features_interacting(degree,x)\n",
    "x_train = multinomial_features(x_train,degree)\n",
    "x_train = zscore_normal(x_train)\n",
    "\n",
    "regressor = PolynomialRegression(x_train,w,b,l,alpha,iterations,)\n",
    "\n",
    "w_final, b_final, J_hist = regressor.grad_descent(y_train)\n",
    "# print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "\n",
    "y_pred = regressor.predict(x_train,w_final,b_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score:  0.9999999999930077\n"
     ]
    }
   ],
   "source": [
    "r2score = r2_score(y_train,y_pred)\n",
    "print(\"R2 Score for train data: \",r2score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=x[24000:]\n",
    "y_test=y[24000:]\n",
    "x_test = np.array(x_test,dtype=float)\n",
    "y_test = np.array(y_test,dtype=float)\n",
    "y_test = y_test.transpose()\n",
    "x_test = multinomial_features(x_test,degree)\n",
    "x_test = zscore_normal(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score:  0.9919622618692064\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = regressor.predict(x_test, w_final, b_final)\n",
    "print(\"R2 Score for test data: \", r2_score(y_test,y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open('Classification_train.csv','r')\n",
    "data=list(csv.reader(file))\n",
    "x=[]\n",
    "y=[]\n",
    "for row in data:\n",
    "    y.append(row[0])\n",
    "for row in data:\n",
    "    x.append(row[1:])\n",
    "file.close()\n",
    "x_train=x[1:24001]\n",
    "y_train=y[1:24001]\n",
    "x_train = np.array(x_train,dtype=float)\n",
    "y_train = np.array(y_train,dtype=float)\n",
    "y_train = y_train.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the given dataset..\n",
      "For 0.000000: Cost  0.02719   \n",
      "For 1.000000: Cost  0.01418   \n",
      "For 2.000000: Cost  0.00503   \n",
      "For 3.000000: Cost  0.04913   \n",
      "For 4.000000: Cost  0.02296   \n",
      "For 5.000000: Cost  0.02262   \n",
      "For 6.000000: Cost  0.02930   \n",
      "For 7.000000: Cost  0.05428   \n",
      "For 8.000000: Cost  0.01285   \n",
      "For 9.000000: Cost  0.01666   \n"
     ]
    }
   ],
   "source": [
    "regressor = LogisticRegression()\n",
    "w=np.zeros(x_train.shape[1])\n",
    "b=0\n",
    "x_train = mean_normal(x_train)\n",
    "\n",
    "iterations = 1000\n",
    "alpha = 5.0\n",
    "w_final, b_final = regressor.fitnew(x_train, y_train, w, b, alpha, iterations)\n",
    "# print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "\n",
    "y_pred = regressor.predictnew(x_train, w_final, b_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for train data: 0.967667\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy for train data: {accuracy(y_pred, y_train):0.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=x[24001:]\n",
    "y_test=y[24001:]\n",
    "x_test = np.array(x_test,dtype=float)\n",
    "y_test = np.array(y_test,dtype=float)\n",
    "y_test = y_test.transpose()\n",
    "x_test = mean_normal(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for test data: 0.959333\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = regressor.predictnew(x_test, w_final, b_final)\n",
    "print(f\"Accuracy for test data: {accuracy(y_test_pred, y_test):0.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
